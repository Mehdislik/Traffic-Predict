# Optional: ensure dependency in Colab
# !pip install -q dateparser

import os
import re
import warnings
import pandas as pd
import dateparser

# ---------------------------
# Configuration
# ---------------------------
INPUT_CSV  = "histo_trafic.csv"
OUTPUT_CSV = "histo_trafic_cleaned.csv"
EXPECTED_COLS = {"secteur", "site", "tstamp", "trafic_mbps"}

# ---------------------------
# 1) Load raw data with safety checks
# ---------------------------
if not os.path.exists(INPUT_CSV):
    raise FileNotFoundError(f"Input file not found: {INPUT_CSV}")

# Try ISO-8859-1 first (as provided), fallback to UTF-8 if needed
try:
    df = pd.read_csv(INPUT_CSV, encoding="ISO-8859-1", sep=";")
except UnicodeDecodeError:
    df = pd.read_csv(INPUT_CSV, encoding="utf-8", sep=";")

# Normalize column names: strip + lowercase
df.columns = df.columns.str.strip().str.lower()

# Check required columns
missing = EXPECTED_COLS - set(df.columns)
if missing:
    raise ValueError(f"Missing required columns: {sorted(missing)}; found: {sorted(df.columns)}")

# Keep only the necessary columns (order as expected)
df = df[["secteur", "site", "tstamp", "trafic_mbps"]]

# ---------------------------
# 2) Basic cleaning
# ---------------------------

# Trim whitespace in string columns
for col in ["secteur", "site", "tstamp", "trafic_mbps"]:
    if col in df.columns and df[col].dtype == object:
        df[col] = df[col].astype(str).str.strip()

# Convert trafic_mbps to numeric:
# - replace decimal comma with dot
# - remove non-digit chars except dot and minus (in case of erroneous negatives)
# - coerce invalids to NaN
def _clean_numeric(x: str) -> str:
    x = x.replace(",", ".")
    # Remove thousands separators/spaces and units (e.g., " Mb/s", " Mbps")
    x = re.sub(r"[^\d\.\-]", "", x)
    return x

df["trafic_mbps"] = pd.to_numeric(df["trafic_mbps"].map(_clean_numeric), errors="coerce")

# Parse timestamps (French-friendly) then convert to pandas datetime
# dateparser handles formats like "01/03/2024 12:00", "mars 2024", etc.
df["tstamp"] = df["tstamp"].apply(lambda x: dateparser.parse(str(x), languages=["fr"]))
df["tstamp"] = pd.to_datetime(df["tstamp"], errors="coerce")

# ---------------------------
# 3) Data validation checks
# ---------------------------

# Drop rows with missing critical fields
before_drop = len(df)
df = df.dropna(subset=["tstamp", "trafic_mbps", "secteur"])
after_drop = len(df)
dropped = before_drop - after_drop
if dropped > 0:
    warnings.warn(f"Dropped {dropped} rows with missing tstamp/trafic_mbps/secteur.")

# Remove negative traffic values (physically invalid)
neg_count = (df["trafic_mbps"] < 0).sum()
if neg_count > 0:
    warnings.warn(f"Found {neg_count} negative trafic_mbps values; removing.")
    df = df[df["trafic_mbps"] >= 0]

# Remove duplicated (secteur, tstamp) pairs, keep the last occurrence
dups = df.duplicated(subset=["secteur", "tstamp"]).sum()
if dups > 0:
    warnings.warn(f"Found {dups} duplicate (secteur, tstamp) rows; keeping last occurrence.")
    df = df.drop_duplicates(subset=["secteur", "tstamp"], keep="last")

# Quick sanity checks
if df.empty:
    raise ValueError("No data left after cleaning. Please inspect the input file.")

unique_sectors = df["secteur"].nunique()
if unique_sectors == 0:
    raise ValueError("No valid 'secteur' entries found after cleaning.")

# ---------------------------
# 4) Sort and save
# ---------------------------
df = df.sort_values(by=["secteur", "tstamp"]).reset_index(drop=True)
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8")

# ---------------------------
# 5) Minimal summary (helpful in logs)
# ---------------------------
print("Cleaned dataset saved:", OUTPUT_CSV)
print("Rows:", len(df), "| Unique sectors:", unique_sectors)
print("Date range:", df["tstamp"].min(), "â†’", df["tstamp"].max())
print(df.head(3))

